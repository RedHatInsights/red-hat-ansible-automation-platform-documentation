[id="ref-resource-types"]

== Resource types

CPU and memory are each a resource type. 
A resource type has a base unit. 
CPU represents compute processing and is specified in units of Kubernetes CPUs. 
Memory is specified in units of bytes. 

CPU and memory are collectively referred to as compute resources, or resources. 
Compute resources are measurable quantities that can be requested, allocated, and consumed. 
They are distinct from API resources. API resources, such as Pods and Service, are objects that can be read and modified through the Kubernetes API server.

== Resource requests and limits of Pod and container
For each container, you can specify resource limits and requests, including the following:

[options="nowrap" subs="+quotes,attributes"]
----
spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
----

Although you can only specify requests and limits for individual containers, it is also useful to think about the overall resource requests and limits for a Pod. 
For a particular resource, a Pod resource request/limit is the sum of the resource requests/limits of that type for each container in the Pod.

== Resource units in Kubernetes

=== CPU resource units

Limits and requests for CPU resources are measured in CPU units. 
In Kubernetes, 1 CPU unit is equivalent to 1 physical processor core, or 1 virtual core, depending on whether the node is a physical host or a virtual machine running inside a physical machine.

Fractional requests are allowed. 
When you define a container with `spec.containers[].resources.requests.cpu` set to `0.5`, you are requesting half as much CPU time compared to if you asked for 1.0 CPU. 
For CPU resource units, the quantity expression 0.1 is equivalent to the expression 100m, which can be read as one hundred millicpu or one hundred millicores. 
This is understood to mean the same thing.
CPU resource is always specified as an absolute amount of resource, never as a relative amount. 
For example, 500m CPU represents the roughly same amount of computing power whether that container runs on a single-core, dual-core, or 48-core machine.

[NOTE]
====
You cannot specify CPU resources with a precision finer than 1m. Because of this, it's useful to specify CPU units less than 1.0 or 1000m using the milliCPU form; for example, 5m rather than 0.005.
====

=== Memory resource units
Limits and requests for memory are measured in bytes. 
You can express memory as a plain integer or as a fixed-point number using one of these quantity suffixes: E, P, T, G, M, k. 
You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. 
For example, the following represent roughly the same value:

[options="nowrap" subs="+quotes,attributes"]
----
128974848, 129e6, 129M,  128974848000m, 123Mi
----

Pay attention to the case of the suffixes. 
If you request 400m of memory, this is a request for 0.4 bytes, not 400 mebibytes (400Mi) or 400 megabytes (400M).

For example, given the resources specified below, the cluster has enough free resources to schedule a task pod with a dedicated 100m CPU and 250Mi, and that cluster can also withstand bursts over that dedicated usage up to 2000m CPU and 2Gi memory.  

[options="nowrap" subs="+quotes,attributes"]
----
spec:
  task_resource_requirements:
    requests:
      cpu: 100m
      memory: 250Mi
    limits:
      cpu: 2000m
      memory: 2Gi
----

Controller will not schedule jobs that use more resources than the limit set. 
If the task pod does use more resources than the limit set, the container is OOMKilled by kubernetes and restarted.  

=== Resource Requests

All jobs that use a container group use the same Pod Specification. 
The Pod Specification includes the resource requests for the pod that runs the job. 

All jobs use the same resource requests. 
The specified resource requests for your particular job on the pod specification affect how Kubernetes schedules the job pod based on resources available on worker nodes. 
These are the default values. 

* In general, 1 fork takes 100Mb of memory. This is set using `system_task_forks_mem`. 
So if your jobs have 5 forks, you would want your job pod specification to request 500Mb of memory.
* If you have job templates that have particularly high forks value or otherwise need larger resource requests, you should create a separate container group with a different pod spec that indicates larger resource requests. 
Then you can assign it to the job template in question. 
For example a job template with the forks value of 50 would be best paired with a container group that requests 5GB of memory. 
* If a job’s forks value is very high, such that no single pod would be able to contain the job, use of the job slicing feature can split the inventory up such that the individual job “slices” each do fit well in an automation pod provisioned by the container group.

