[id="con-aap-upgrade-prereq_{context}"]

= Prerequisites for upgrading {PlatformNameShort}

Before you begin to upgrade {PlatformNameShort}, ensure your environment meets the following node and configuration requirements.

== Node requirements

The following specifications are required for the nodes involved in the {PlatformNameShort} upgrade process:

* 16 GB of RAM for controller nodes, database node, execution nodes and hop nodes.
* 4 CPUs for controller nodes, database nodes, execution nodes, and hop nodes.
* 150 GB+ disk space for database node.
* 40 GB+ disk space for non-database nodes.
* DHCP reservations use infinite leases to deploy the cluster with static IP addresses.
* DNS records for all nodes.
* Red Hat Enterprise Linux 8 or later 64-bit (x86) installed for all nodes.
* Chrony configured for all nodes.
* Python 3.8 or later for all content dependencies.

== {ControllerNameStart} configuration requirements
The following {ControllerName} configurations are required before you proceed with the {PlatformNameShort} upgrade process:

.Configuring NTP server using Chrony

Each {PlatformNameShort} node in the cluster must have access to an NTP server. Use the `chronyd` to synchronize the system clock with NTP servers. This ensures that cluster nodes using SSL certificates that require validation do not fail if the date and time between nodes are not in sync.

This is required for all nodes used in the upgraded {PlatformNameShort} cluster:

. Install `chrony`:
+
----
# dnf install chrony --assumeyes
----
. Open `/etc/chrony.conf` using a text editor.
. Locate the public server pool section and modify it to include the appropriate NTP server addresses. Only one server is required, but three are recommended. Add the 'iburst' option to speed up the time it takes to properly sync with the servers:
+
----
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
server <ntp-server-address> iburst
----
. Save changes within the `/etc/chrony.conf` file.
. Start the host and enable the `chronyd` daemon:
+
----
# systemctl --now enable chronyd.service
----
. Verify the `chronyd` daemon status:
+
----
# systemctl status chronyd.service
----

.Attaching Red Hat subscription on all nodes
{PlatformName} requires you to have valid subscriptions attached to all nodes. You can verify that your current node has a Red Hat subscription by running the following command:
----
# subscription-manager list --consumed
----
If there is not a Red Hat subscription attached to the node, see https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/{PlatformVers}/html/red_hat_ansible_automation_platform_installation_guide/planning-installation#proc-attaching-subscriptions_planning[attaching your {PlatformNameShort} subscription] for more information.

.Creating non-root user with sudo privileges
Before you upgrade {PlatformNameShort}, it is recommended to create a non-root user with sudo privileges for the deployment process. This user is used for:

* SSH connectivity.
* Passwordless authentication during installation.
* Privilege escalation (sudo) permissions.

The following example uses `ansible` to name this user. On all nodes used in the upgraded {PlatformNameShort} cluster, create a non-root user named `ansible` and generate an ssh key:

. Create a non-root user:
+
----
# useradd ansible
----
. Set a password for your user:
+
----
# passwd ansible <1>
Changing password for ansible.
Old Password:
New Password:
Retype New Password:
----
<1> Replace `ansible` with the non-root user from step 1, if using a different name
. Generate an `ssh` key as the user:
+
----
$ ssh-keygen -t rsa
----
. Disable password requirements when using `sudo`:
+
----
# echo "ansible ALL=(ALL) NOPASSWD:ALL" | sudo tee -a /etc/sudoers.d/ansible
----

.Copying SSH keys to all nodes
With the `ansible` user created, copy the `ssh` key to all the nodes used in the upgraded {PlatformNameShort} cluster. This ensures that when the {PlatformNameShort} installation runs, it can `ssh` to all the nodes without a password:
----
$ ssh-copy-id ansible@node-1.example.com
----
NOTE: If running within a cloud provider, you might need to instead create an `~/.ssh/authorized_keys` file containing the public key for the `ansible` user on all your nodes and set the permissions to the `authorized_keys` file to only the owner (`ansible`) having read and write access (permissions 600).

.Configuring firewall settings
Configure the firewall settings on all the nodes used in the upgraded {PlatformNameShort} cluster to permit access to the appropriate services and ports for a successful {PlatformNameShort} upgrade. For Red Hat Enterprise Linux 8 or later, enable the `firewalld` daemon to enable the access needed for all nodes:

. Install the `firewalld` package:
+
----
# dnf install firewalld --assumeyes
----
. Start the `firewalld` service:
+
----
# systemctl start firewalld
----
. Enable the `firewalld` service:
+
----
# systemctl enable --now firewalld
----


== {PlatformNameShort} configuration requirements
The following {PlatformNameShort} configurations are required before you proceed with the {PlatformNameShort} upgrade process:

.Configuring firewall settings for execution and hop nodes

After upgrading your {PlatformName} instance, add the automation mesh port on the mesh nodes (execution and hop nodes) to enable automation mesh functionality. The default port used for the mesh networks on all nodes is `27199/tcp`. You can configure the mesh network to use a different port by specifying `recptor_listener_port` as the variable for each node within your inventory file.

Within your hop and execution node set the `firewalld` port to be used for installation.

. Ensure that `firewalld` is running:
+
----
$ sudo systemctl status firewalld
----
. Add the `firewalld` port to your controller database node (e.g. port 27199):
+
----
$ sudo firewall-cmd --permanent --zone=public --add-port=27199/tcp
----
. Reload `firewalld`:
+
----
$ sudo firewall-cmd --reload
----
. Confirm that the port is open:
+
----
$ sudo firewall-cmd --list-ports
----
